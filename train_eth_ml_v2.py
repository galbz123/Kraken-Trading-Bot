#!/usr/bin/env python
"""
ETH ML Training Script (V2 - Upgraded SMC Features)
====================================================
Trains ETH ML model on historical data using new SMC features:
- HTF Bullish Bias (EMA50 > EMA200, Close > EMA50)
- BOS (Break of Structure)
- Liquidity Sweeps
- Order Block Proximity
- Discount Zone
- FVG (Fair Value Gaps)

Uses 5m LTF (Low Timeframe) for features and 15m HTF (High Timeframe) for bias.
"""

import os
import sys
import ccxt
import pandas as pd
import numpy as np
from dotenv import load_dotenv
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import cross_val_score
import joblib
import datetime

print("=" * 80)
print("ðŸŽ“ ETH ML TRAINING SIMULATION (V2 - Upgraded SMC Features)")
print("=" * 80)

# Load environment
load_dotenv()

api_key = os.getenv('API_KEY')
secret = os.getenv('SECRET')

if not api_key or not secret:
    print("âŒ Error: API_KEY and SECRET must be set in .env file")
    sys.exit(1)

# Initialize exchange
exchange = ccxt.krakenfutures({
    'apiKey': api_key,
    'secret': secret,
    'enableRateLimit': True,
})

# ===== INDICATOR FUNCTIONS =====
def calculate_ema(series, period):
    """Calculate EMA"""
    return series.ewm(span=period, adjust=False).mean()

def calculate_rsi(series, period=14):
    """Calculate RSI"""
    delta = series.diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
    rs = gain / loss
    rsi = 100 - (100 / (1 + rs))
    return rsi

def find_pivot_highs(df, window=3):
    """Find pivot highs (local maxima)"""
    highs = df['high'].rolling(window=window*2+1, center=True).apply(
        lambda x: x[window] if x[window] == max(x) else np.nan, raw=True
    )
    return highs

def find_pivot_lows(df, window=3):
    """Find pivot lows (local minima)"""
    lows = df['low'].rolling(window=window*2+1, center=True).apply(
        lambda x: x[window] if x[window] == min(x) else np.nan, raw=True
    )
    return lows

def detect_break_of_structure_v2(df, lookback=20, window=3):
    """Detect bullish BOS using pivot points"""
    if df is None or len(df) < lookback + window * 2:
        return False
    
    pivot_highs = find_pivot_highs(df, window=window)
    pivot_lows = find_pivot_lows(df, window=window)
    
    recent_pivot_highs = pivot_highs.dropna().tail(lookback)
    recent_pivot_lows = pivot_lows.dropna().tail(lookback)
    
    if len(recent_pivot_highs) == 0 or len(recent_pivot_lows) == 0:
        return False
    
    last_pivot_high = recent_pivot_highs.iloc[-1]
    last_pivot_low = recent_pivot_lows.iloc[-1]
    
    current_high = df['high'].iloc[-1]
    current_low = df['low'].iloc[-1]
    
    # Bullish BOS: breaks above pivot high
    if current_high > last_pivot_high:
        return True
    return False

def detect_liquidity_sweeps_v2(df, lookback=30, window=3):
    """Detect liquidity sweep lows using pivot points"""
    if df is None or len(df) < lookback + window * 2:
        return False
    
    pivot_lows = find_pivot_lows(df, window=window)
    recent_pivot_lows = pivot_lows.dropna().tail(10)
    
    if len(recent_pivot_lows) == 0:
        return False
    
    last_pivot_low = recent_pivot_lows.iloc[-1]
    
    # Check last 3 candles for sweep
    for i in range(1, min(4, len(df))):
        candle = df.iloc[-i]
        current_close = df['close'].iloc[-1]
        
        if candle['low'] < last_pivot_low * 0.999 and current_close > last_pivot_low:
            return True
    
    return False

def detect_order_blocks_v2(df, lookback=50):
    """Detect bullish order blocks"""
    if df is None or len(df) < lookback:
        return []
    
    recent = df.tail(lookback)
    order_blocks = []
    
    for i in range(1, len(recent) - 2):
        curr = recent.iloc[i]
        next_bar = recent.iloc[i+1]
        
        avg_vol = recent.iloc[max(0, i-10):i]['volume'].mean()
        vol_condition = curr['volume'] > avg_vol * 1.5
        
        body_size = abs(curr['close'] - curr['open'])
        move_condition = body_size > (curr['high'] - curr['low']) * 0.6
        
        if vol_condition and move_condition:
            if curr['close'] > curr['open'] and next_bar['close'] > curr['close']:
                order_blocks.append({
                    'type': 'bullish_ob',
                    'low': curr['low'],
                    'high': curr['high'],
                    'mid': (curr['low'] + curr['high']) / 2,
                })
    
    return order_blocks

def check_order_block_proximity_v2(df, order_blocks, tolerance=0.02):
    """Check if price near order block"""
    if not order_blocks:
        return False
    
    current_price = df['close'].iloc[-1]
    
    for ob in order_blocks:
        if ob['type'] == 'bullish_ob':
            ob_low_range = ob['low'] * (1 - tolerance)
            ob_high_range = ob['high'] * (1 + tolerance)
            
            if ob_low_range <= current_price <= ob_high_range:
                return True
    
    return False

def detect_premium_discount_zones_v2(df, lookback=100):
    """Detect discount zones"""
    if df is None or len(df) < lookback:
        return 'neutral'
    
    recent = df.tail(lookback)
    
    high = recent['high'].max()
    low = recent['low'].min()
    range_size = high - low
    
    current = df['close'].iloc[-1]
    discount_threshold = low + (range_size * 0.30)
    
    if current < discount_threshold:
        return 'in_discount'
    else:
        return 'neutral'

def detect_fair_value_gaps_v2(df, lookback=30):
    """Detect FVG (Fair Value Gaps)"""
    if df is None or len(df) < lookback:
        return []
    
    recent = df.tail(lookback)
    fvgs = []
    
    for i in range(1, len(recent) - 1):
        prev = recent.iloc[i-1]
        curr = recent.iloc[i]
        
        # Bullish FVG: gap up
        if prev['high'] < curr['low']:
            gap_size = curr['low'] - prev['high']
            if gap_size > prev['close'] * 0.001:
                fvgs.append({
                    'type': 'bullish_fvg',
                    'top': curr['low'],
                    'bottom': prev['high'],
                    'mid': (curr['low'] + prev['high']) / 2,
                })
    
    return fvgs

def calculate_smc_features_v2(df_htf, df_ltf):
    """
    Calculate SMC features from HTF and LTF data
    Returns: dict of 6 features
    """
    features = {
        'htf_bias': 0,
        'bos': 0,
        'sweep': 0,
        'ob_proximity': 0,
        'discount_zone': 0,
        'fvg': 0
    }
    
    # 1. HTF Bullish Bias
    if df_htf is not None and len(df_htf) >= 200:
        ema50 = calculate_ema(df_htf['close'], 50)
        ema200 = calculate_ema(df_htf['close'], 200)
        close_htf = df_htf['close'].iloc[-1]
        
        if ema50.iloc[-1] > ema200.iloc[-1] and close_htf > ema50.iloc[-1]:
            features['htf_bias'] = 1
    
    # 2. BOS Bullish
    if detect_break_of_structure_v2(df_ltf):
        features['bos'] = 1
    
    # 3. Liquidity Sweep
    if detect_liquidity_sweeps_v2(df_ltf):
        features['sweep'] = 1
    
    # 4. Order Block Proximity
    obs = detect_order_blocks_v2(df_ltf)
    if check_order_block_proximity_v2(df_ltf, obs):
        features['ob_proximity'] = 1
    
    # 5. Discount Zone
    if detect_premium_discount_zones_v2(df_ltf) == 'in_discount':
        features['discount_zone'] = 1
    
    # 6. FVG
    current_price = df_ltf['close'].iloc[-1]
    fvgs = detect_fair_value_gaps_v2(df_ltf)
    for fvg in fvgs:
        if fvg['type'] == 'bullish_fvg':
            if fvg['bottom'] * 0.98 <= current_price <= fvg['top'] * 1.02:
                features['fvg'] = 1
                break
    
    return features

# ===== MAIN TRAINING =====

print("\nðŸ“Š Fetching historical data...")
print(f"   LTF: 5m (entry timeframe)")
print(f"   HTF: 15m (context timeframe)")

# Fetch 5m data (LTF) - get 720 candles = 2.5 days
try:
    ohlcv_ltf = exchange.fetch_ohlcv('ETH/USD:USD', '5m', limit=720)
    df_ltf_all = pd.DataFrame(ohlcv_ltf, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
    df_ltf_all['timestamp'] = pd.to_datetime(df_ltf_all['timestamp'], unit='ms')
    print(f"   âœ“ Fetched {len(df_ltf_all)} 5m candles")
except Exception as e:
    print(f"   âŒ Failed to fetch LTF data: {e}")
    sys.exit(1)

# Fetch 15m data (HTF) - get 720 candles = 7.5 days
try:
    ohlcv_htf = exchange.fetch_ohlcv('ETH/USD:USD', '15m', limit=720)
    df_htf_all = pd.DataFrame(ohlcv_htf, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
    df_htf_all['timestamp'] = pd.to_datetime(df_htf_all['timestamp'], unit='ms')
    print(f"   âœ“ Fetched {len(df_htf_all)} 15m candles")
except Exception as e:
    print(f"   âŒ Failed to fetch HTF data: {e}")
    sys.exit(1)

print("\nðŸ” Extracting SMC features from historical data...")

X = []
y = []
feature_count = 0
label_1_count = 0
label_0_count = 0

# For each LTF candle, calculate SMC features
# Use HTF data for context (just the end window)
htf_context = df_htf_all.iloc[-200:]  # Last 200 HTF candles for context

for idx in range(5, len(df_ltf_all) - 5):  # Leave room for lookback and lookahead
    df_ltf_window = df_ltf_all.iloc[max(0, idx-50):idx+1]  # 50 candles lookback
    
    # Calculate features
    features = calculate_smc_features_v2(htf_context, df_ltf_window)
    
    # Label: 1 if next 5 candles avg close is higher, 0 otherwise
    next_5_close = df_ltf_all.iloc[idx+1:idx+6]['close'].values
    if len(next_5_close) >= 5:
        future_return = (next_5_close[-1] - df_ltf_all.iloc[idx]['close']) / df_ltf_all.iloc[idx]['close']
        label = 1 if future_return > 0 else 0
        
        X.append([features['htf_bias'], features['bos'], features['sweep'], 
                  features['ob_proximity'], features['discount_zone'], features['fvg']])
        y.append(label)
        
        feature_count += 1
        if label == 1:
            label_1_count += 1
        else:
            label_0_count += 1

print(f"   âœ“ Extracted {feature_count} training samples")
print(f"   âœ“ Label distribution: {label_1_count} positive ({label_1_count/feature_count*100:.1f}%), {label_0_count} negative ({label_0_count/feature_count*100:.1f}%)")

if feature_count < 100:
    print(f"   âš ï¸  Warning: Only {feature_count} samples, need at least 100")

# Convert to numpy
X = np.array(X)
y = np.array(y)

print("\nðŸ¤– Training ML model...")

# Create pipeline with StandardScaler
pipeline = make_pipeline(
    StandardScaler(),
    LogisticRegression(max_iter=1000, random_state=42)
)

# Train
pipeline.fit(X, y)

# Cross-validation
scores = cross_val_score(pipeline, X, y, cv=5, scoring='accuracy')
print(f"   âœ“ Model trained")
print(f"   âœ“ Cross-validation accuracy: {scores.mean():.3f} Â± {scores.std():.3f}")
print(f"   âœ“ Scores by fold: {[f'{s:.3f}' for s in scores]}")

# Save model
model_path = 'logs/ml_model_eth.joblib'
joblib.dump(pipeline, model_path)
print(f"\nðŸ’¾ Model saved: {model_path}")

print("\n" + "=" * 80)
print("âœ… TRAINING COMPLETE!")
print("=" * 80)
print(f"Model is ready to use. The bot will:")
print(f"  1. Load this trained model")
print(f"  2. Use it for predictions on live data")
print(f"  3. Retrain every 30 min with new data")
print(f"  4. Gradually replace historical samples with real trades")
print("\nâ±ï¸  Expected improvement timeline:")
print(f"  - Week 1: 46% accuracy â†’ ~50% (baseline)")
print(f"  - Week 2: 50% â†’ ~55% (real trades entering)")
print(f"  - Week 3: 55% â†’ ~60% (model learning)")
print(f"  - Week 4+: 60%+ (institutional patterns)")
print("=" * 80)
